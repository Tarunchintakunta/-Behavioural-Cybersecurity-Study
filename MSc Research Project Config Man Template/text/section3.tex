\section{User Guide}
\label{sec:user_guide}

This section explains how to run the various components of the project pipeline.

\subsection{1. Data Preprocessing}
The first step is to clean and prepare the raw survey and experiment data. Run the following command:

\begin{verbatim}
python src/processing/preprocess_for_modeling.py
\end{verbatim}

This script reads the raw data from \texttt{data/raw/}, cleans it, handles missing values, and saves the processed dataset to \texttt{data/processed/model\_training\_data.csv}.

\subsection{2. Statistical Analysis}
To perform the statistical analysis (ANOVA, t-tests, correlations) as defined in the research methodology:

\begin{verbatim}
python src/analysis/statistical_analysis.py
\end{verbatim}

The results will be saved as a JSON report in \texttt{results/statistical\_analysis\_report.json}.

\subsection{3. Model Training}
To train the machine learning models (Logistic Regression, Random Forest, etc.) and evaluate their performance:

\begin{verbatim}
python src/models/train_vulnerability_predictor.py
\end{verbatim}

This script trains multiple models, logs metrics to MLflow, and saves the best performing model to \texttt{models/saved/}.

\subsubsection{Model Specifications}
The following machine learning models are implemented in \texttt{src/models/train\_vulnerability\_predictor.py} to predict phishing susceptibility:

\begin{itemize}
    \item \textbf{Logistic Regression}: A baseline linear model used for its interpretability and efficiency.
    \item \textbf{Random Forest Classifier}: An ensemble learning method that operates by constructing a multitude of decision trees at training time.
    \item \textbf{Gradient Boosting Classifier}: A boosting technique that builds models sequentially to correct the errors of previous models.
    \item \textbf{Support Vector Machine (SVM)}: A powerful classifier effective in high-dimensional spaces, used here with probability estimation enabled.
\end{itemize}

\subsubsection{Implementation Details}
The models are initialized with specific configurations as shown in the code snippet below. The \texttt{random\_state} is set from the configuration file to ensure reproducibility.

\begin{verbatim}
models = {
    "LogisticRegression": LogisticRegression(
        random_state=self.config['ml_models']['random_state'], 
        max_iter=1000
    ),
    "RandomForest": RandomForestClassifier(
        random_state=self.config['ml_models']['random_state']
    ),
    "GradientBoosting": GradientBoostingClassifier(
        random_state=self.config['ml_models']['random_state']
    ),
    "SVM": SVC(
        probability=True, 
        random_state=self.config['ml_models']['random_state']
    )
}
\end{verbatim}

The training process involves splitting the data into training and testing sets, followed by cross-validation to evaluate model performance using ROC AUC scoring. The best performing model is then selected and saved.

\subsection{4. Interactive Dashboard}
To explore the data, view analysis results, and test the model interactively, launch the Streamlit dashboard:

\begin{verbatim}
streamlit run src/visualization/dashboard.py
\end{verbatim}

This will open a web interface in your default browser (usually at \texttt{http://localhost:8501}). Use the sidebar to navigate between "Exploratory Data Analysis", "Statistical Insights", and "ML Model Performance".

