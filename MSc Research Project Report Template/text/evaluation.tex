\section{Evaluation}
\label{sec:evaluation}

This chapter presents the results of the data analysis and model evaluation. It addresses the research questions posed in Chapter 1 by interpreting the statistical findings and assessing the performance of the machine learning models.

\subsection{Statistical Analysis Results}

\subsubsection{Exploratory Data Analysis}
Before hypothesis testing, we analyzed the distributions of key variables. Figure \ref{fig:age_dist} shows the age distribution of participants, while Figure \ref{fig:vuln_dist} illustrates the distribution of vulnerability scores.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/eda_age_dist.png}
    \caption{Age Distribution of Participants}
    \label{fig:age_dist}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/eda_vulnerability_dist.png}
    \caption{Distribution of Vulnerability Scores}
    \label{fig:vuln_dist}
\end{figure}

To understand the relationships between variables, a correlation heatmap was generated (Figure \ref{fig:heatmap}).

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/eda_correlation_heatmap.png}
    \caption{Correlation Heatmap of Key Variables}
    \label{fig:heatmap}
\end{figure}

\subsubsection{RQ1: Cognitive Biases and Vulnerability}
A Pearson correlation analysis was conducted to examine the relationship between cognitive bias scores and phishing vulnerability. The analysis revealed a weak negative correlation ($r = -0.161$). This suggests that individuals with higher cognitive bias scores (indicating higher susceptibility to biases like optimism) were slightly \textit{less} likely to be vulnerable in this specific dataset, which contradicts the initial hypothesis. This unexpected result may be due to the specific nature of the biases measured or the synthetic nature of the data.

\subsubsection{RQ2: Stress and Multitasking}
The analysis of situational factors yielded significant findings.
\begin{itemize}
    \item \textbf{Stress}: A strong positive correlation ($r = 0.632$) was found between perceived stress levels and vulnerability. This confirms the hypothesis that high stress depletes cognitive resources, making individuals more prone to errors. Figure \ref{fig:stress_analysis} visualizes this relationship.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/stats_stress_analysis.png}
        \caption{Impact of Stress Levels on Vulnerability}
        \label{fig:stress_analysis}
    \end{figure}
    \item \textbf{Multitasking}: The correlation between multitasking habits and vulnerability was negligible ($r = -0.036$). This suggests that general multitasking tendencies may not directly translate to immediate phishing susceptibility, or that the measure used (Media Multitasking Index) did not capture the specific type of task switching relevant to email processing.
\end{itemize}

\subsubsection{RQ3: Impact of Prior Training}
An Independent Samples T-test was performed to compare the mean vulnerability scores of participants with and without prior cybersecurity training.
\begin{itemize}
    \item \textbf{Mean Vulnerability (Trained)}: 0.419
    \item \textbf{Mean Vulnerability (Untrained)}: 0.474
    \item \textbf{P-value}: 0.2216
\end{itemize}
The difference was not statistically significant ($p > 0.05$). This indicates that, in this study, prior training did not provide a measurable protective effect. This finding aligns with literature suggesting that traditional "tick-box" training is often ineffective at changing behavior, as noted by \cite{Reinheimer2020} and \cite{Aldawood2024}. Figure \ref{fig:training_effect} illustrates this comparison.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/training_effect_visualization.png}
    \caption{Effect of Prior Training on Vulnerability}
    \label{fig:training_effect}
\end{figure}

\subsubsection{RQ4: Demographic Factors (Job Role)}
An ANOVA test was conducted to compare vulnerability across different job roles. The result was highly significant ($F = 62.71$, $p < 0.001$), indicating that job role is a strong predictor of vulnerability. This supports the need for role-specific security interventions, consistent with findings by \cite{Alhogbani2023}.

\subsection{Machine Learning Model Performance}
The four machine learning models were evaluated based on their ability to predict the binary vulnerability outcome. The performance comparison is visualized in Figure \ref{fig:ml_comparison}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ml_performance_comparison.png}
    \caption{Comparison of Model Performance Metrics (Accuracy, Precision, Recall, F1)}
    \label{fig:ml_comparison}
\end{figure}

\subsubsection{Cross-Validation Results}
The Support Vector Machine (SVM) achieved the highest mean ROC AUC score during cross-validation, outperforming the tree-based models. This suggests that the SVM's ability to find a hyperplane in high-dimensional space was better suited for the complex, non-linear relationships between psychological traits and vulnerability.

\begin{table}[h]
    \centering
    \caption{Cross-Validation ROC AUC Scores}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Model} & \textbf{Mean CV ROC AUC} \\
        \hline
        Logistic Regression & 0.3532 \\
        Random Forest & 0.2661 \\
        Gradient Boosting & 0.2509 \\
        \textbf{SVM} & \textbf{0.3824} \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{Test Set Evaluation and Overfitting Analysis}
The best-performing model (SVM) was evaluated on the held-out test set. The ROC curve is shown in Figure \ref{fig:roc_curve}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/ml_roc_curve.png}
    \caption{ROC Curve for SVM Model}
    \label{fig:roc_curve}
\end{figure}

\begin{table}[h]
    \centering
    \caption{Test Set Metrics (SVM)}
    \begin{tabular}{|l|c|}
        \hline
        \textbf{Metric} & \textbf{Score} \\
        \hline
        Accuracy & 92.11\% \\
        Precision & 0.00\% \\
        Recall & 0.00\% \\
        ROC AUC & 0.4476 \\
        \hline
    \end{tabular}
\end{table}

\textbf{Analysis of Overfitting and Class Imbalance}:
The discrepancy between the high accuracy (92.11\%) and the zero precision/recall scores indicates a severe class imbalance issue. The dataset contains a significantly higher number of "Resilient" users compared to "Vulnerable" ones. As a result, the model has learned to maximize accuracy by simply predicting the majority class (Resilient) for every instance.

This is further evidenced by the Confusion Matrix in Figure \ref{fig:confusion_matrix}, which shows that the model failed to correctly identify any of the actual vulnerable users (True Positives = 0).

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/ml_confusion_matrix.png}
    \caption{Confusion Matrix for SVM Model}
    \label{fig:confusion_matrix}
\end{figure}

\subsubsection{Feature Importance}
To understand which factors contributed most to the model's decisions (even if biased), we analyzed feature importance using SHAP values (for tree-based models) and coefficient magnitude (for linear models). Figure \ref{fig:feature_importance} illustrates the relative importance of features.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ml_feature_importance.png}
    \caption{Feature Importance Analysis}
    \label{fig:feature_importance}
\end{figure}

To provide a deeper understanding of model decisions, SHAP (SHapley Additive exPlanations) values were calculated. Figure \ref{fig:shap} shows the SHAP summary plot, highlighting the direction and magnitude of feature impact.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/shap_summary.png}
    \caption{SHAP Summary Plot}
    \label{fig:shap}
\end{figure}

As expected from the statistical analysis, \textbf{Stress Level} was a top predictor. However, demographic features like \textbf{Job Role} also played a significant role, confirming that certain organizational functions are inherently more targeted or vulnerable. The low predictive power of the \textbf{Training} feature reinforces the finding that current training methods are not a strong differentiator for security behavior.

\subsection{Discussion of Results}
The high accuracy (92\%) combined with 0\% precision and recall indicates a severe class imbalance problemâ€”the model is simply predicting the majority class (Resilient) for everyone. While the SVM showed some discriminative power in cross-validation (ROC AUC > 0.5 is random, but here it's lower, suggesting potential inverse correlation or noise), the overall predictive performance is limited.

The strong statistical correlation with stress ($r=0.63$) suggests that stress is a key feature. However, the machine learning models struggled to leverage this, possibly due to the dominance of non-predictive features or the small sample size. The significant finding regarding job roles suggests that future models should weigh this feature more heavily or use hierarchical modeling.
