"""
Vulnerability Prediction ML Model
Predicts phishing susceptibility using machine learning
"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from sklearn.preprocessing import LabelEncoder
import joblib
import yaml
import logging
import os
import sys
import matplotlib.pyplot as plt
import mlflow

# Add the project root to the Python path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

# Correctly import from sibling modules
from src.models.model_evaluator import plot_confusion_matrix, plot_roc_curve
from src.utils.experiment_tracking import get_tracker

# --- Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class VulnerabilityPredictor:
    def __init__(self, config_path: str):
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        self.model = None
        self.tracker = get_tracker("Phishing Vulnerability Prediction")
        logging.info("Configuration loaded and experiment tracker initialized.")

    def prepare_data(self, data_path: str):
        logging.info(f"Loading data from {data_path}")
        df = pd.read_csv(data_path)
        
        feature_cols = self.config['ml_models']['feature_columns']
        target_col = self.config['ml_models']['target_variable']

        # Ensure all feature columns exist
        feature_cols = [col for col in feature_cols if col in df.columns]
        self.config['ml_models']['feature_columns'] = feature_cols # Update config
        
        # Handle categorical features by label encoding
        categorical_cols = [col for col in feature_cols if df[col].dtype == 'object']
        for col in categorical_cols:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col].astype(str))
            logging.info(f"Label encoded column: {col}")

        X = df[feature_cols].copy()
        y = df[target_col]

        # Final check for and imputation of NaNs
        for col in X.columns:
            if X[col].isnull().any():
                if pd.api.types.is_numeric_dtype(X[col]):
                    # If the entire column is NaN, median() will be NaN. Fill with 0 in that case.
                    fill_value = X[col].median()
                    if pd.isna(fill_value):
                        fill_value = 0
                else:
                    # mode()[0] is safe even if there are NaNs
                    fill_value = X[col].mode()[0]
        
                X[col].fillna(fill_value, inplace=True)
                logging.info(f"Filled NaNs in column '{col}' with {fill_value}.")
        
        logging.info(f"Data prepared. Features shape: {X.shape}, Target shape: {y.shape}")
        return X, y
    
    def train_and_evaluate(self, X: pd.DataFrame, y: pd.Series):
        with self.tracker as run:
            run.log_params(self.config['ml_models'])

            X_train, X_test, y_train, y_test = train_test_split(
                X, y,
                test_size=self.config['ml_models']['test_size'],
                random_state=self.config['ml_models']['random_state'],
                stratify=y
            )
            logging.info("Data split into training and testing sets.")

            models = {
                "LogisticRegression": LogisticRegression(random_state=self.config['ml_models']['random_state'], max_iter=1000),
                "RandomForest": RandomForestClassifier(random_state=self.config['ml_models']['random_state']),
                "GradientBoosting": GradientBoostingClassifier(random_state=self.config['ml_models']['random_state']),
                "SVM": SVC(probability=True, random_state=self.config['ml_models']['random_state'])
            }
            
            self.trained_models = {}

            best_model_name = ""
            best_model_score = 0.0

            for name, model in models.items():
                logging.info(f"Evaluating {name} with cross-validation...")
                try:
                    cv_scores = cross_val_score(model, X_train, y_train, cv=self.config['ml_models']['cross_validation_folds'], scoring='roc_auc')
                    mean_cv_score = cv_scores.mean()
                    run.log_metric(f"cv_auc_{name}", mean_cv_score)
                    logging.info(f"{name} - CV ROC AUC: {mean_cv_score:.4f}")
                    if mean_cv_score > best_model_score:
                        best_model_score = mean_cv_score
                        best_model_name = name
                except Exception as e:
                    logging.error(f"Could not evaluate {name}. Error: {e}")
                    continue
                
                # Train the model on the full training set and store it
                model.fit(X_train, y_train)
                self.trained_models[name] = model
                logging.info(f"Trained and stored {name}.")

            if not best_model_name:
                logging.error("No model could be successfully evaluated. Aborting.")
                return

            self.model = self.trained_models[best_model_name]
            run.log_param("best_model", best_model_name)
            logging.info(f"Best model selected: {best_model_name}")

            y_pred = self.model.predict(X_test)
            y_pred_proba = self.model.predict_proba(X_test)[:, 1]

            metrics = {
                "test_accuracy": accuracy_score(y_test, y_pred),
                "test_precision": precision_score(y_test, y_pred, zero_division=0),
                "test_recall": recall_score(y_test, y_pred, zero_division=0),
                "test_roc_auc": roc_auc_score(y_test, y_pred_proba)
            }
            run.log_metrics(metrics)
            logging.info(f"Test Set Metrics: {metrics}")

            cm_fig = plot_confusion_matrix(y_test, y_pred, classes=self.model.classes_)
            run.log_figure(cm_fig, "confusion_matrix.png")
            plt.close(cm_fig)
            
            roc_fig = plot_roc_curve(y_test, y_pred_proba)
            run.log_figure(roc_fig, "roc_curve.png")
            plt.close(roc_fig)
            
            logging.info("Logged evaluation plots to MLflow.")
            # Log the best model with an input example to create a signature
            mlflow.sklearn.log_model(
                sk_model=self.model,
                artifact_path="model",
                input_example=X_train.head() # Use a sample of the training data
            )

    def save_models(self, path: str):
        """Saves all trained models to a directory."""
        if self.trained_models:
            os.makedirs(path, exist_ok=True)
            for name, model in self.trained_models.items():
                model_path = os.path.join(path, f"{name.replace(' ', '_')}.joblib")
                joblib.dump(model, model_path)
                logging.info(f"Model '{name}' saved to {model_path}")

def main():
    logging.info("=" * 50)
    logging.info("STARTING MODEL TRAINING PIPELINE")
    
    # Use relative paths from the project root
    config_path = 'config/config.yaml'
    data_path = 'data/processed/model_training_data.csv'
    models_output_path = 'models/saved/' # Directory to save models

    try:
        predictor = VulnerabilityPredictor(config_path)
        X, y = predictor.prepare_data(data_path)
        
        if y.nunique() < 2:
            logging.error("Target variable has only one class. Cannot train a classifier.")
            return

        predictor.train_and_evaluate(X, y)
        predictor.save_models(models_output_path)
        
        logging.info("MODEL TRAINING PIPELINE COMPLETED SUCCESSFULLY")
        logging.info("=" * 50)
    except Exception as e:
        logging.error(f"An error occurred during the model training pipeline: {e}", exc_info=True)

if __name__ == "__main__":
    main()
