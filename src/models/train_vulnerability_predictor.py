"""
Vulnerability Prediction ML Model
Predicts phishing susceptibility using machine learning
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (classification_report, confusion_matrix, 
                            roc_auc_score, roc_curve, precision_recall_curve)
from imblearn.over_sampling import SMOTE
import joblib
import yaml
from typing import Dict, Tuple, List
import matplotlib.pyplot as plt
import seaborn as sns
import sys
import os
import mlflow
import bisect
import logging

# --- Setup Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Add src directory to path to import utils
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from utils.experiment_tracking import get_tracker


class VulnerabilityPredictor:
    """
    Machine Learning model for predicting phishing vulnerability
    """
    
    def __init__(self, config_path: str = 'config/config.yaml'):
        """Initialize predictor with configuration"""
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        self.models = {}
        self.best_model = None
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_importance = None
    
    def plot_confusion_matrix(self, y_test, y_pred, output_path='results/figures/confusion_matrix.png'):
        """Generates and saves a confusion matrix plot."""
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Not Vulnerable', 'Vulnerable'],
                    yticklabels=['Not Vulnerable', 'Vulnerable'])
        plt.title('Confusion Matrix')
        plt.ylabel('Actual')
        plt.xlabel('Predicted')
        
        # Ensure directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        plt.savefig(output_path)
        plt.close()
        print(f"Confusion matrix saved to {output_path}")

    def plot_roc_curve(self, y_test, y_pred_proba, output_path='results/figures/roc_curve.png'):
        """Generates and saves an ROC curve plot."""
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        auc = roc_auc_score(y_test, y_pred_proba)
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {auc:.2f})')
        plt.plot([0, 1], [0, 1], color='red', linestyle='--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC) Curve')
        plt.legend(loc="lower right")

        # Ensure directory exists
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        plt.savefig(output_path)
        plt.close()
        print(f"ROC curve saved to {output_path}")

    def prepare_data(self, data_path: str) -> Tuple[pd.DataFrame, pd.Series]:
        """
        Load the pre-processed, model-ready data.
        """
        logging.info(f"Loading and preparing data from {data_path}...")
        df = pd.read_csv(data_path)
        
        # Define target variable
        target_col = self.config['ml_models']['target_variable']
        df['vulnerable'] = (df[target_col] == 0).astype(int)
        
        # Feature selection from config
        feature_cols = self.config['ml_models']['feature_columns']
        
        # Ensure all feature columns exist, fill missing with median/mode
        for col in feature_cols:
            if col not in df.columns:
                print(f"Warning: Feature column '{col}' not found in data. Removing from list.")
                feature_cols.remove(col)
                continue # Continue to next iteration
            
            if df[col].isnull().any():
                if pd.api.types.is_numeric_dtype(df[col]):
                    df[col].fillna(df[col].median(), inplace=True)
                else:
                    df[col].fillna(df[col].mode()[0], inplace=True)

        # Encode categorical variables
        for col in df.select_dtypes(include=['object']).columns:
            if col in feature_cols:
                if col not in self.label_encoders:
                    self.label_encoders[col] = LabelEncoder()
                    df[col] = self.label_encoders[col].fit_transform(df[col])
                else:
                    # Handle unseen labels during prediction
                    df[col] = df[col].map(lambda s: '<unknown>' if s not in self.label_encoders[col].classes_ else s)
                    le_classes = self.label_encoders[col].classes_.tolist()
                    bisect.insort_left(le_classes, '<unknown>')
                    self.label_encoders[col].classes_ = np.array(le_classes)
                    df[col] = self.label_encoders[col].transform(df[col])
        
        X = df[feature_cols].copy()
        y = df['vulnerable']

        # --- FINAL NaN CHECK ---
        # Ensure no NaNs are left before returning
        for col in X.columns:
            if X[col].isnull().any():
                if pd.api.types.is_numeric_dtype(X[col]):
                    X[col] = X[col].fillna(X[col].median())
                else:
                    X[col] = X[col].fillna(X[col].mode()[0])
        # --- END FINAL NaN CHECK ---

        logging.info(f"Features shape: {X.shape}")
        logging.info(f"Target distribution: \n{y.value_counts()}")
        
        return X, y
    
    def train_models(self, X_train: pd.DataFrame, y_train: pd.Series) -> Dict:
        """
        Train multiple ML models
        """
        logging.info("Training machine learning models...")
        
        # Handle class imbalance with SMOTE
        logging.info("Handling class imbalance with SMOTE...")
        smote = SMOTE(random_state=self.config['ml_models']['random_state'])
        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
        
        logging.info(f"Data shape after SMOTE: {X_train_balanced.shape}")
        
        # Scale features
        logging.info("Scaling features...")
        X_train_scaled = self.scaler.fit_transform(X_train_balanced)
        
        models_config = {
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=self.config['ml_models']['random_state'],
                n_jobs=-1
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=5,
                random_state=self.config['ml_models']['random_state']
            ),
            'logistic_regression': LogisticRegression(
                max_iter=1000,
                random_state=self.config['ml_models']['random_state']
            ),
            'svm': SVC(
                kernel='rbf',
                probability=True,
                random_state=self.config['ml_models']['random_state']
            )
        }
        
        results = {}
        
        for name, model in models_config.items():
            logging.info(f"Training {name}...")
            
            # Train model
            model.fit(X_train_scaled, y_train_balanced)
            
            # Cross-validation
            cv_scores = cross_val_score(
                model, X_train_scaled, y_train_balanced,
                cv=self.config['ml_models']['cross_validation_folds'],
                scoring='roc_auc'
            )
            
            self.models[name] = model
            
            results[name] = {
                'cv_mean': float(cv_scores.mean()),
                'cv_std': float(cv_scores.std()),
                'model': model
            }
            
            logging.info(f"{name} CV ROC-AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
        
        # Select best model
        best_model_name = max(results, key=lambda k: results[k]['cv_mean'])
        self.best_model = self.models[best_model_name]
        
        logging.info(f"Best model selected: {best_model_name}")
        
        return results
    
    def evaluate_model(self, X_test: pd.DataFrame, y_test: pd.Series) -> Dict:
        """
        Evaluate the best model on test set
        """
        logging.info("Evaluating best model on test set...")
        
        X_test_scaled = self.scaler.transform(X_test)
        
        # Predictions
        y_pred = self.best_model.predict(X_test_scaled)
        y_pred_proba = self.best_model.predict_proba(X_test_scaled)[:, 1]
        
        # Generate and save plots
        self.plot_confusion_matrix(y_test, y_pred)
        self.plot_roc_curve(y_test, y_pred_proba)

        # Metrics
        cm = confusion_matrix(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        
        # Classification report
        report = classification_report(y_test, y_pred, output_dict=True)
        
        results = {
            'confusion_matrix': cm.tolist(),
            'roc_auc': float(roc_auc),
            'classification_report': report,
            'predictions': y_pred.tolist(),
            'probabilities': y_pred_proba.tolist()
        }
        
        logging.info(f"Test Set ROC-AUC: {roc_auc:.4f}")
        logging.info(f"Classification Report:\n{classification_report(y_test, y_pred)}")
        
        return results
    
    def get_feature_importance(self, feature_names: list) -> pd.DataFrame:
        """
        Get feature importance from the best model
        """
        if hasattr(self.best_model, 'feature_importances_'):
            importance = self.best_model.feature_importances_
            
            self.feature_importance = pd.DataFrame({
                'feature': feature_names,
                'importance': importance
            }).sort_values('importance', ascending=False)
            
            print("\nTop 10 Most Important Features:")
            print(self.feature_importance.head(10))
            
            return self.feature_importance
        else:
            print("Feature importance not available for this model type")
            return None
    
    def save_model(self, model_path: str = 'models/saved/vulnerability_predictor.joblib'):
        """
        Save trained model and preprocessors
        """
        model_package = {
            'model': self.best_model,
            'scaler': self.scaler,
            'label_encoders': self.label_encoders,
            'feature_importance': self.feature_importance
        }
        
        joblib.dump(model_package, model_path)
        logging.info(f"Model saved to {model_path}")
    
    def load_model(self, model_path: str = 'models/saved/vulnerability_predictor.joblib'):
        """
        Load saved model and preprocessors
        """
        model_package = joblib.load(model_path)
        
        self.best_model = model_package['model']
        self.scaler = model_package['scaler']
        self.label_encoders = model_package['label_encoders']
        self.feature_importance = model_package['feature_importance']
        
        logging.info(f"Model loaded from {model_path}")
    
    def predict_vulnerability(self, features: Dict) -> Dict:
        """
        Predict vulnerability for a single individual
        
        Args:
            features: Dictionary of individual's features
        
        Returns:
            Prediction result with probability
        """
        # Convert to DataFrame
        X = pd.DataFrame([features])
        
        # Encode categorical features
        for col, encoder in self.label_encoders.items():
            if col in X.columns:
                X[col] = encoder.transform(X[col])
        
        # Scale features
        X_scaled = self.scaler.transform(X)
        
        # Predict
        prediction = self.best_model.predict(X_scaled)[0]
        probability = self.best_model.predict_proba(X_scaled)[0]
        
        risk_level = "HIGH" if probability[1] > 0.7 else "MEDIUM" if probability[1] > 0.4 else "LOW"
        
        result = {
            'vulnerable': bool(prediction),
            'vulnerability_probability': float(probability[1]),
            'risk_level': risk_level,
            'recommendations': self._generate_recommendations(features, probability[1])
        }
        
        return result
    
    def _generate_recommendations(self, features: Dict, vulnerability_score: float) -> List[str]:
        """
        Generate personalized recommendations based on vulnerability profile
        """
        recommendations = []
        
        if vulnerability_score > 0.7:
            recommendations.append("URGENT: High vulnerability detected. Immediate training recommended.")
            recommendations.append("Enable email filters and security warnings.")
        elif vulnerability_score > 0.4:
            recommendations.append("Moderate risk. Regular training sessions recommended.")
        else:
            recommendations.append("Low risk. Maintain current security awareness.")
        
        # Personalized based on features
        if features.get('response_time_seconds', 60) < 20:
            recommendations.append("Take more time to evaluate emails before acting.")
        
        return recommendations


def main():
    """Main function for training vulnerability predictor"""
    logging.info("=" * 60)
    logging.info("Starting Phishing Vulnerability Prediction Model Training")
    logging.info("=" * 60)
    
    # Get MLflow experiment tracker
    tracker = get_tracker("phishing_vulnerability_models")
    
    # Start MLflow run
    with tracker.start_run(run_name="vulnerability_predictor_training"):
        predictor = VulnerabilityPredictor()
        
        try:
            # Load and prepare data from the single, preprocessed file
            X, y = predictor.prepare_data('data/processed/model_training_data.csv')
            
            # Split data
            logging.info("Splitting data into training and testing sets...")
            X_train, X_test, y_train, y_test = train_test_split(
                X, y,
                test_size=predictor.config['ml_models']['test_size'],
                random_state=predictor.config['ml_models']['random_state'],
                stratify=y
            )
            
            logging.info(f"Train set shape: {X_train.shape}")
            logging.info(f"Test set shape: {X_test.shape}")
            
            # Log dataset info
            tracker.log_params({
                "data_shape": X.shape,
                "train_size": X_train.shape[0],
                "test_size": X_test.shape[0],
                "feature_count": X.shape[1],
                "class_distribution": str(y.value_counts().to_dict())
            })
            
            # Train models
            training_results = predictor.train_models(X_train, y_train)
            
            # Log model parameters
            for model_name, result in training_results.items():
                model_params = result['model'].get_params()
                for param_name, param_value in model_params.items():
                    tracker.log_params({f"{model_name}_{param_name}": str(param_value)})
                
                # Log cross validation scores
                tracker.log_metrics({
                    f"{model_name}_cv_roc_auc_mean": result['cv_mean'],
                    f"{model_name}_cv_roc_auc_std": result['cv_std']
                })
            
            # Evaluate best model
            evaluation_results = predictor.evaluate_model(X_test, y_test)
            
            # Log evaluation metrics
            tracker.log_metrics({
                "test_roc_auc": evaluation_results['roc_auc'],
                "test_accuracy": evaluation_results['classification_report']['accuracy'],
                "test_precision": evaluation_results['classification_report']['weighted avg']['precision'],
                "test_recall": evaluation_results['classification_report']['weighted avg']['recall'],
                "test_f1": evaluation_results['classification_report']['weighted avg']['f1-score']
            })
            
            # Feature importance
            feature_importance_df = predictor.get_feature_importance(X.columns.tolist())
            
            # Log feature importance
            if feature_importance_df is not None:
                for _, row in feature_importance_df.iterrows():
                    tracker.log_metrics({
                        f"feature_importance_{row['feature']}": row['importance']
                    })
                
                # Create and log feature importance plot
                plt.figure(figsize=(10, 8))
                sns.barplot(x='importance', y='feature', data=feature_importance_df.head(15))
                plt.title('Feature Importance')
                plt.tight_layout()
                tracker.log_figure(plt.gcf(), "feature_importance.png")
                plt.close()
            
            # Log evaluation plots
            tracker.log_artifact('results/figures/confusion_matrix.png')
            tracker.log_artifact('results/figures/roc_curve.png')

            # Save model
            predictor.save_model()
            
            # Log the model to MLflow
            tracker.log_model(predictor.best_model, "vulnerability_predictor")
            
            logging.info("=" * 60)
            logging.info("âœ“ Model training complete!")
            logging.info(f"Model tracked with MLflow - run ID: {mlflow.active_run().info.run_id}")
            logging.info("=" * 60)
            
        except FileNotFoundError as e:
            logging.error("Model training data not found. Please run 'src/processing/preprocess_for_modeling.py' first.")
            logging.error(f"Details: {e}")
            
            # Log the error
            tracker.set_tags({"status": "failed", "error": str(e)})


if __name__ == "__main__":
    main()
